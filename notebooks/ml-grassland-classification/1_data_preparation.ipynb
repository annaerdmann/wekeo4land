{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../media/common/LogoWekeo_Copernicus_RGB_0.png' align='left' height='96px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input data for training a supervised learning model\n",
    "\n",
    "*Authors: Adrian Di Paolo, Chung-Xiang Hong, Jonas Viehweger* \n",
    "\n",
    "This notebook shows the steps towards preparing data for training a supervised machine learning model. We will train the model based on the [pan-European High Resolution Vegetation Phenology and Productiviy (HR-VPP)](https://collections.sentinel-hub.com/vegetation-phenology-and-productivity-parameters-season-1/) data, which is derived from ESA’s Sentinel-2 as part of the Copernicus Land Monitoring Service (CLMS). The 14 parameters that describe specific stages of the seasonal vegetation growth cycle will be used as input features to fit a model.\n",
    "\n",
    "As for the ground truth, we will use the [EuroCrops](https://github.com/maja601/EuroCrops#vectordata_zenodo) data, which is a dataset combining all publicly available self-declared crop reporting datasets from countries of the European Union.\n",
    "\n",
    "In this example notebook, the expected outcome is a binary classifier that identifies grassland and non-grassland areas in the Netherlands. We will start by preparing a small dataset to train models with different algorithms, so we can have an overview on the performance of the models that is essential in the following model selection process.\n",
    "\n",
    "Preparing the data at small scale for training a model can be simple and straightforward with the following three steps:\n",
    "1. [Define training and validation area](#1-define-training-and-validation-area)\n",
    "2. [Obtain features and labels](#2-obtain-features-and-labels)\n",
    "3. [Reshape data for model training](#3-reshape-data-for-model-training)\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "Note about Environment: There is an `environment.yml` included, with which all the necessary libraries can be installed into a conda environment. They can be installed using  \n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "# Jupyter notebook related\n",
    "%matplotlib inline\n",
    "\n",
    "# Built-in modules\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.enums import Resampling\n",
    "import mypy\n",
    "from typing import Union\n",
    "\n",
    "# Visualisation\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data access\n",
    "from hda import Client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define training and validation area\n",
    "\n",
    "To train a machine learning model, we need to define a training dataset and a validation dataset. The training data set is a set of example data which is used to fit the parameters (e.g., weights) during the learning process. Then, the trained model needs to be evaluated using examples from the held-out dataset, which is the validation dataset that is independent of the training dataset and is not used in the training process.\n",
    "\n",
    "In our case, we will randomly select a training area and a validation area in the Netherlands to obtain examples for the training and the evaluation process. First let's take a look at these two areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bounding box\n",
    "bounds_poly = gpd.read_file(\"../../data/raw/ml-grassland-classification/bounding_boxes.geojson\")\n",
    "validation_gdf = bounds_poly[-1:]\n",
    "train_gdf = bounds_poly[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdf_bbox(gdf: gpd.GeoDataFrame) -> list:\n",
    "    \"\"\"This returns the bounding box \n",
    "    of all geometries in a geodataframe\"\"\"\n",
    "    return list(gdf.total_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These bounding boxes are needed to later query the EuroCrops dataset\n",
    "total_bbox = gdf_bbox(bounds_poly.to_crs(4326))\n",
    "validation_bbox = gdf_bbox(validation_gdf.to_crs(4326))\n",
    "train_bbox = gdf_bbox(train_gdf.to_crs(4326))\n",
    "\n",
    "print(train_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_map = folium.Map(location=(53.104538500000004, 7.014084), zoom_start=8) \n",
    "folium.GeoJson(data=validation_gdf['geometry'], style_function=lambda x:{\"color\": \"red\"}).add_to(_map)\n",
    "folium.GeoJson(data=train_gdf['geometry'], style_function=lambda x:{\"color\": \"blue\"}).add_to(_map)\n",
    "_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the ground truth dataset\n",
    "\n",
    "Since we are going to train a binary classification model to identify if an area is grassland or not in the Netherlands, it is important to investigate the distribution of crop types in the country.  \n",
    "\n",
    "Here we display the distribution of the top 10 categories reported in the Netherlands. We can see that grassland is the most dominant category in the data, which is a critical point we should pay attention to when sampling data for training at a national scale.\n",
    "\n",
    "**Note** that [EuroCrops Netherlands](https://zenodo.org/record/7476474/files/NL_2020.zip?download=1) data needs to be downloaded and saved in the same folder where this notebook is located.\n",
    "\n",
    "This is also quite a big dataset, so loading it entirely may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "wget \"https://zenodo.org/record/7476474/files/NL_2020.zip?download=1\" -O ../../data/download/ml-grassland-classification/NL_2020.zip &&\n",
    "unzip -o ../../data/download/ml-grassland-classification/NL_2020.zip -d ../../data/processing/ml-grassland-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full dataset to GeoDataFrame\n",
    "EC_NETHERLANDS_PATH = \"../../data/processing/ml-grassland-classification/NL/NL_2020_EC21.shp\"\n",
    "netherlands_gt = gpd.read_file(EC_NETHERLANDS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netherlands_gt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort catagories by value counts\n",
    "categories = netherlands_gt.EC_hcat_n.value_counts(sort=True)\n",
    "\n",
    "# create a subset for the top 10 catagories\n",
    "categories_subset = categories[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pie chart\n",
    "categories_subset.plot.pie(figsize=(15,8), autopct='%1.1f%%');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's have a closer look at the data in the training area and the validation area. We can see the distribution of crop types is much more balanced in both the training area and validation area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "train_gt = gpd.read_file(EC_NETHERLANDS_PATH, bbox=tuple(train_bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all geometries in the training area\n",
    "train_gt.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pie chart\n",
    "train_gt.EC_hcat_n.value_counts(sort=True)[:10].plot.pie(figsize=(15,8), autopct='%1.1f%%');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load validation dataset\n",
    "validation_gt = gpd.read_file(EC_NETHERLANDS_PATH, bbox=tuple(validation_bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_gt.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_gt.EC_hcat_n.value_counts(sort=True)[:10].plot.pie(figsize=(15,8), autopct='%1.1f%%');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "Training and validating a model with samples in small areas will introduce a bias. The bias comes from two aspects:\n",
    "* The imbalanced samples of grassland and non-grassland\n",
    "* The spatial correlation between samples\n",
    "\n",
    "This will be addressed later when preparing data at a large scale to train a model general enough for the whole country."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtain HR-VPP data and labels\n",
    "\n",
    "In total we will show two possibilities to get HR-VPP data. The first is by using the HDA API which is available on WEkEO. This API has a few limitations which necessitate a bit more post-processing of the data. In a later notebook we will show how to access the data using the Sentinelhub API which is more capable in its processing.\n",
    "\n",
    "### Downloading Data\n",
    "\n",
    "First the data is downloaded from WEkEO using the HDA python library. A getting started tutorial for the API is available [here](https://help.wekeo.eu/en/articles/6751608-what-is-the-hda-api-python-client-and-how-to-use-it). \n",
    "\n",
    "For the download we specify a helper function which subsets the data query to only a single tile. This is done to reduce the amount of data that will be downloaded, since a lot of tiles are overlapping the study area. The HDA API will always download entire tiles even if you are only interested in small parts of the tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hda_tile_download(client: Client, query: dict, tile: str, folder: Union[str, os.PathLike]) -> None:\n",
    "    # The following line runs the query\n",
    "    matches = client.search(query)\n",
    "    # This subsets the search results, so only results in the specified tile are included\n",
    "    matches_subset = [match for match in matches.results if tile in match[\"properties\"][\"location\"]]\n",
    "    matches.results = matches_subset\n",
    "    matches.download(folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the query we specify which dataset we want to download, for which timeframe and for which area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = Client(progress=True)\n",
    "\n",
    "for product_type in ['AMPL', 'EOSD', 'EOSV', 'LENGTH', 'LSLOPE', 'MAXD', 'MAXV', 'MINV', 'QFLAG', 'RSLOPE', 'SOSD', 'SOSV', 'SPROD', 'TPROD']:\n",
    "\n",
    "  query = {\n",
    "    \"dataset_id\": \"EO:EEA:DAT:CLMS_HRVPP_VPP\",\n",
    "    \"productType\": product_type,\n",
    "    \"productVersion\": \"V101\",\n",
    "    \"productGroupId\": \"s1\",\n",
    "    \"start\": \"2020-01-01T00:00:00.000Z\",\n",
    "    \"end\": \"2020-01-01T00:00:00.000Z\",\n",
    "    \"bbox\": total_bbox\n",
    "  }\n",
    "\n",
    "  hda_tile_download(c, query, tile=\"T32ULD\", folder=\"../../data/download/ml-grassland-classification/hda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = list(Path(\"../../data/download/ml-grassland-classification/hda/\").glob(\"*.tif\"))\n",
    "file_list.sort()\n",
    "file_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDA API downloads different HR-VPP parameters in separate TIFF files. So the next step is to read in the data for our training and validation areas and stack them together. To ensure you download all the necessary parameters, remember to adjust the “productType” with the specific parameter IDs: `'AMPL', 'EOSD', 'EOSV', 'LENGTH', 'LSLOPE', 'MAXD', 'MAXV', 'MINV', 'QFLAG', 'RSLOPE', 'SOSD', 'SOSV', 'SPROD', 'TPROD'`. This will ensure the correct set of HR-VPP data is included for your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_hrvpp(file_list: list, gdf: gpd.GeoDataFrame) -> np.ndarray:\n",
    "    arrays = []\n",
    "    bounds = gdf_bbox(gdf)\n",
    "    for file in file_list:\n",
    "        with rio.open(file) as src:\n",
    "            assert gdf.crs == src.crs\n",
    "            transform = src.transform\n",
    "            window = from_bounds(*bounds, transform).round_offsets().round_lengths()\n",
    "            arrays.append(src.read(1, window=window))\n",
    "    bounds = rio.windows.bounds(window, transform)\n",
    "    new_transform = rio.transform.from_bounds(*bounds, window.width, window.height)\n",
    "    return (np.stack(arrays, axis=-1), new_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_transform = stack_hrvpp(file_list, train_gdf)\n",
    "validation_data, validation_transform = stack_hrvpp(file_list, validation_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two arrays, one for the training area and one for the validation area, which is smaller than the one for the training area. But both arrays now have all 14 bands as their third dimension."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels\n",
    "\n",
    "We have the HR-VPP data of both training and validation areas now. Next, we want to get the labels for these two areas as the ground truth to perform a supervised training.\n",
    "\n",
    "To do this, we have to convert the vector data to raster data. This is done with the `get_labels` function which sets up the data for the `rasterio` function `rasterize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crs(tiff_path):\n",
    "    with rio.open(tiff_path) as src:\n",
    "        crs = src.crs\n",
    "    return crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = get_crs(file_list[0])\n",
    "print(crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get labels\n",
    "def get_labels(ec_gt: gpd.GeoDataFrame, \n",
    "               geotransform, \n",
    "               out_shape: Union[tuple, list], \n",
    "               raster_crs: rio.crs.CRS) -> np.ndarray:\n",
    "    ec_gt[\"label\"] = ec_gt.EC_hcat_c.astype(float)\n",
    "    geo_iter = list(ec_gt.to_crs(32632)[[\"geometry\", \"label\"]].itertuples(index=False, name=None))\n",
    "    rasterized = rasterize(geo_iter, transform=geotransform, out_shape=out_shape, fill=-1)\n",
    "    return rasterized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = get_labels(train_gt, train_transform, train_data.shape[0:2], crs)\n",
    "print(train_labels.shape)\n",
    "\n",
    "validation_labels = get_labels(validation_gt, validation_transform, validation_data.shape[0:2], crs)\n",
    "\n",
    "def plot_raster_with_legend(moj_rast):\n",
    "    # Plot the raster data\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(moj_rast, cmap='viridis', interpolation='none')\n",
    "    plt.colorbar(label='Raster Value')  # Add a color bar (legend)\n",
    "    plt.title('Raster Plot with Legend')\n",
    "    plt.xlabel('Column Index')\n",
    "    plt.ylabel('Row Index')\n",
    "    plt.show()\n",
    "\n",
    "plot_raster_with_legend(train_labels)\n",
    "plot_raster_with_legend(validation_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reshape data for model training\n",
    "\n",
    "At this point we have all the data we need to train a supervised machine learning model. The last step is to reshape the data so that it can be fed into the model. In general, we simply flatten the arrays to go from 3 dimensional data to 2 dimensional data, where the first dimension are the pixels and the second dimension the values of the bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input(features: np.ndarray, labels: np.ndarray) -> tuple:\n",
    "    x = np.reshape(features, (-1, features.shape[2]))\n",
    "    y = labels.flatten()\n",
    "    x_clean = x[y!=-1,:]\n",
    "    y_clean = y[y!=-1]\n",
    "    return x_clean, y_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = get_model_input(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation, y_validation = get_model_input(validation_data, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'x_validation': x_validation, \n",
    "    'y_validation': y_validation,\n",
    "    'x_data': x_data,\n",
    "    'y_data': y_data\n",
    "}\n",
    "Path(\"../../data/processing/ml-grassland-classification/dataset\").mkdir(exist_ok=True)\n",
    "for name, dataset in datasets.items():\n",
    "    np.save(Path(\"../../data/processing/ml-grassland-classification/dataset/\", f\"{name}.npy\"), dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When preparing the input data for a supervised learning task, there are a few important points which can be the takeaways from this notebook:  \n",
    "1. Ground truth data is critical for a supervised learning task. It determines how well a model can learn from your data and the reliability of the validation result.\n",
    "2. It is important to have two separate dataset. One for training and the other for validation. Always make sure that the validation dataset is held-out in the training process.\n",
    "3. Training with a limited amount of samples can lead to bias. The bias can be alleviated if there are sufficient amounts of samples.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
